{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will solve the reacher environment - a task based on continuous control. We will solve it with policy-based deep reinforcement learning.\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting of the Reacher environment (see README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "Performing random actions to check the Python API to control the agent and receive feedback from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DDPG\n",
    "\n",
    "Training an agent with ddpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../utils/ddpg_agent.py:113: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 4.49\n",
      "Episode 200\tAverage Score: 13.65\n",
      "Episode 300\tAverage Score: 17.39\n",
      "Episode 400\tAverage Score: 17.42\n",
      "Episode 500\tAverage Score: 17.76\n",
      "Episode 600\tAverage Score: 18.09\n",
      "Episode 700\tAverage Score: 20.38\n",
      "Episode 800\tAverage Score: 46.60\n",
      "Episode 900\tAverage Score: 51.41\n",
      "Episode 968\tAverage Score: 52.34"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-339bcccc5307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-339bcccc5307>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/rl-control/utils/ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/rl-control/utils/replay_buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drlnd/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.ddpg_agent import Agent\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=random_seed)\n",
    "\n",
    "def ddpg(n_episodes=5000, max_t=2000, print_every=100):\n",
    "    best_score = 30\n",
    "    scores_window = deque(maxlen=print_every)\n",
    "    scores_all = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        agent.reset()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations           \n",
    "        scores = np.zeros(num_agents)\n",
    "\n",
    "        for _ in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            rewards = env_info.rewards\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            scores += rewards\n",
    "            states = next_states\n",
    "                \n",
    "        avg_score = np.mean(scores)\n",
    "        avg_score = np.mean(scores)\n",
    "        scores_window.append(avg_score)\n",
    "        scores_all.append(avg_score)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "    return scores_all\n",
    "\n",
    "scores = ddpg()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_scores = [4.49, 13.65, 17.39, 17.42, 17.76, 18.09, 20.38, 46.60, 51.41, 52.34]\n",
    "episode_nr = [100, 200, 300, 400, 500, 600, 700, 800, 900, 968]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(episode_nr, avg_scores)\n",
    "plt.ylabel('Avg. Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
